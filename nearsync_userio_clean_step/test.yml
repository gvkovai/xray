name: NearSync_UserIO_Clean_Step

display_name: "NEARSYNC UserIO Clean Step"

summary:
  Data Protection NearSync DR

description: |
  <strong>What's this test about?</strong><br/>
  Disaster Recovery - Protection Domain - NearSync DR
  Iterations:
  <br/><br/>
  <br/><br/>
  <strong>How to measure the impact of Compression on Wire on snapshots?</strong><br/>
  <br/><br/>
  <strong>How is the test run?</strong><br/>
  Setup
  <ol>
    <li> Cluster stop.  Modify GFLAGS.  Cluster start.</li>
    <li> Deploy the VM template. </li>
    <li> Pre-fill the VM(s). </li>
    <li> Create Protection Domain </li>
    <li> Take Snapshot </li>
    <li> Take measurements </li>
  </ol>
  Measurement
  <ol>
        <li>replicated_bytes</li>
        <li>tx_bytes</li>
        <li>uncompressed_tx_bytes</li>
        <li>Duration</li>
  </ol>
  Test Requirements
  <ul class="indented">
    <li>vCPUs: 4 vCPUs per VM</li>
    <li>RAM: 4 GB per VM</li>
    <li>IP Addresses: 1 per VM</li>
  </ul>

tags:
  - performance

presets:
  random_writes_44:
    vars:
      _preset_name: "Random Writes 44"
      A_vms_per_node: 1
      B_disks_per_vm: 8
      D_read_percent: 0
      E_random_percent: 100
      F_iodepth: 16
      G_block_size: 8192
      H_target_rate: 20000
      I_runtime: 1800
      K_steps: 5
      L_pd_name_prefix: "random_writes"
      N_start_trate: 4000
      O_wait4replications: 180
      P_pd_name: "NS1"
      Q_pgip: "10.56.4.85"
      R_nearsync_interval: 10
      S_remote: "RTP-Test-46"
      pg_username: "admin"
      pg_passwd: "Nutanix/4u$"
      M_wait4oplogdrain: 60
      M_timeout4oplogdrain: 1800
      tgt_pgip: "10.56.4.90"
      src_cvms: "10.56.4.81,10.56.4.82,10.56.4.83,10.56.4.84"
      tgt_cvms: "10.56.4.86,10.56.4.87,10.56.4.88,10.56.4.89"
  sequential_writes_44:
    vars:
      _preset_name: "Sequential Writes 44"
      A_vms_per_node: 1
      B_disks_per_vm: 8
      D_read_percent: 0
      E_random_percent: 0
      F_iodepth: 16
      G_block_size: 1048576
      H_target_rate: 300
      I_runtime: 1800
      K_steps: 5
      L_pd_name_prefix: "sequential_writes"
      N_start_trate: 60
      O_wait4replications: 180
      P_pd_name: "NS1"
      Q_pgip: "10.56.4.85"
      R_nearsync_interval: 10
      S_remote: "RTP-Test-46"
      pg_username: "admin"
      pg_passwd: "Nutanix/4u$"
      M_wait4oplogdrain: 60
      M_timeout4oplogdrain: 1800
      tgt_pgip: "10.56.4.90"
      src_cvms: "10.56.4.81,10.56.4.82,10.56.4.83,10.56.4.84"
      tgt_cvms: "10.56.4.86,10.56.4.87,10.56.4.88,10.56.4.89"
  random_writes_63:
    vars:
      _preset_name: "Random Writes 63"
      A_vms_per_node: 1
      B_disks_per_vm: 8
      D_read_percent: 0
      E_random_percent: 100
      F_iodepth: 16
      G_block_size: 8192
      H_target_rate: 20000
      I_runtime: 1800
      K_steps: 5
      L_pd_name_prefix: "random_writes"
      N_start_trate: 4000
      O_wait4replications: 180
      P_pd_name: "NS1"
      Q_pgip: "10.56.4.130"
      R_nearsync_interval: 10
      S_remote: "RTP-Test-41"
      pg_username: "admin"
      pg_passwd: "Nutanix/4u$"
      M_wait4oplogdrain: 60
      M_timeout4oplogdrain: 1800
      tgt_pgip: "10.56.64.70"
      src_cvms: "10.56.4.126,10.56.4.127,10.56.4.128,10.56.4.129"
      tgt_cvms: "10.56.64.66,10.56.64.67,10.56.64.68,10.56.64.69"
  sequential_writes_63:
    vars:
      _preset_name: "Sequential Writes 63"
      A_vms_per_node: 1
      B_disks_per_vm: 8
      D_read_percent: 0
      E_random_percent: 0
      F_iodepth: 16
      G_block_size: 1048576
      H_target_rate: 300
      I_runtime: 1800
      K_steps: 5
      L_pd_name_prefix: "sequential_writes"
      N_start_trate: 60
      O_wait4replications: 180
      P_pd_name: "NS1"
      Q_pgip: "10.56.4.130"
      R_nearsync_interval: 10
      S_remote: "RTP-Test-41"
      pg_username: "admin"
      pg_passwd: "Nutanix/4u$"
      M_wait4oplogdrain: 60
      M_timeout4oplogdrain: 1800
      tgt_pgip: "10.56.64.70"
      src_cvms: "10.56.4.126,10.56.4.127,10.56.4.128,10.56.4.129"
      tgt_cvms: "10.56.64.66,10.56.64.67,10.56.64.68,10.56.64.69"

vars:
  A_vms_per_node:
    display_name: "vms_per_node"
    default: 1
    min: 1
    max: 4
  B_disks_per_vm:
    description: |
      The number of disks attached to each VM.
    default: 8
    display_name: "Number of disks attached to each VM"
    min: 1
    max: 16
  C_vm_wss:
    description: |
      The VM working set size in megabytes.
      The working set size is a measurement of how much
      disk data the application is operating on for the duration of the test.
    default: 196608
    display_name: "VM working set size (MB)"
    min: 200
  D_read_percent:
    description: |
      The workload read percent.
      Setting read percent to 0 results in full writes.
      Setting read percent to 100 results in full reads.
    default: 50
    display_name: "Workload read percent"
    min: 0
    max: 100
  E_random_percent:
    description: |
      The workload random percent.
      Setting random percent to 0 results in a fully sequential I/O workload.
      Setting random percent to 100 results in a fully random I/O
      workload.
    default: 50
    display_name: "Workload random percent"
    min: 0
    max: 100
  F_iodepth:
    description: |
      The number of I/O operations left outstanding per disk.
    default: 16
    display_name: "Number of I/O operations left outstanding per disk"
    min: 1
  G_block_size:
    description: |
      The workload block size in bytes.
    default: 8192
    display_name: "Workload block size in bytes"
    min: 512
  H_target_rate:
    description: |
      The target IOPS rate per VM (0 for unlimited).
    default: 0
    display_name: "Target IOPS rate per VM (0 for unlimited)"
    min: 0
  I_runtime:
    description: |
      The workload runtime in seconds.
    default: 900
    display_name: "Runtime in seconds"
    min: 60
  J_num_nodes:
    description: |
      Number of nodes in the cluster.
    default: 4
    display_name: "Number of nodes in the cluster"
    min: 4
  K_steps:
    description: |
      Number of run steps
    default: 5
    display_name: "Number of run steps"
  L_pd_name_prefix:
    description: |
      Prefix for protection domain name.
    default: "pd_name_prefix"
    display_name: "Prefix for protection domain name"
  M_wait4oplogdrain:
    description: |
      Min time to wait after oplog/lws drained
    default: 60
    display_name: "Time to wait for oplog/lws drained"
    min: 0
  M_timeout4oplogdrain:
    description: |
      Max time to wait for oplog/lws to be drained
    default: 1800
    display_name: "Max Time to wait for oplog/lws to be drained"
    min: 0
  N_start_trate:
    description: |
      Starting target rate
    default: 4000
    display_name: "Starting target rate"
  O_wait4replications:
    description: |
      Min time to wait for replications to be completed
    default: 180
    display_name: "Time to wait for replications to be completed"
    min: 0
  P_pd_name:
    description: |
      protection domain name.
    default: "NS1"
    display_name: "protection domain name"
  Q_pgip:
    description: |
      Prism gateway IP.
    default: "10.56.4.85"
    display_name: "Prism Gateway IP"
  tgt_pgip:
    description: |
      Target Prism gateway IP.
    default: "10.56.4.90"
    display_name: "Target Prism Gateway IP"
  R_nearsync_interval:
    description: |
      Nearsync interval in minutes.
    default: 10
    max: 15
    display_name: "Nearsync Interval in Minutes"
  S_remote:
    description: |
      Nearsync remote cluster name.
    default: "RTP-Test-46"
    display_name: "Remote Cluster Name for NearSync target"
  pg_username:
    description: |
      Prism Gateway Username.
    default: "admin"
    display_name: "Prism Gateway Username"
  pg_passwd:
    description: |
      Prism Gateway Password.
    default: "Nutanix/4u$"
    display_name: "Prism Gateway Password"
  number_of_curator_scans:
    display_name: "Number of Curator Scans"
    default: 3
    min: 1
    max: 9
  tgt_cvms:
    description: |
      Target CVMs.
    default: "10.56.4.86,10.56.4.87,10.56.4.88,10.56.4.89"
    display_name: "Comma separated IPs of target CVMs"    
  src_cvms:
    description: |
      Source CVMs.
    default: "10.56.4.81,10.56.4.82,10.56.4.83,10.56.4.84"
    display_name: "Comma separated IPs of source CVMs"    
  fresh_start:
    description: |
      Determines whether to call fresh start.
    display_name: "Call fresh start (curator scans)"
    default: true 
  cluster_cleanup:
    description: |
      Determines whether to call the cluster.Cleanup step.
      Do not select if you want existing X-Ray UVMs to be left on the cluster.
    display_name: "Call cluster.Cleanup"
    default: true 
  set_gflags:
    description: |
      Do you want to set gflags?
    default: true
    display_name: "Do you want to set gflags?"

{% set H_target_rate = H_target_rate|int %}
{% set N_start_trate = N_start_trate|int %}
{% set M_wait4oplogdrain = M_wait4oplogdrain |int %}

{% if H_target_rate > 0 %}

{% if M_wait4oplogdrain > 0 %}
estimated_runtime: {{ (H_target_rate/K_steps)*I_runtime + (H_target_rate/K_steps)*M_wait4oplogdrain + 3600 }}
{% else %}
estimated_runtime: {{ (H_target_rate/K_steps)*I_runtime + 3600 }}
{% endif %}

{% else %}
estimated_runtime: {{ I_runtime + 3600 }}
{% endif %}


vms:
  - group_0:
      template: ubuntu1604
      vcpus: 4
      ram_mb: 4096
      data_disks:
        count: {{ B_disks_per_vm }}
        size: 512
      nodes: all
      count_per_node: {{ A_vms_per_node }}

workloads:
  - Custom Workload Prefill:
      vm_group: group_0
      config_file: custom.fio
      config_variables:
        disk_size: {{ (C_vm_wss / B_disks_per_vm) | int }}
        disk_count: {{ B_disks_per_vm }}
        read_percent: 0
        random_percent: 0
        iodepth: {{ (128 / B_disks_per_vm) | int }}
        block_size: 1m
{% if H_target_rate > 0 %}
{% for s_target_rate in range(N_start_trate, H_target_rate+K_steps, (H_target_rate/K_steps)|int) %}
  - {{ s_target_rate }} IOPS:
      vm_group: group_0
      config_file: custom.fio
      config_variables:
        disk_size: {{ (C_vm_wss / B_disks_per_vm) | int }}
        disk_count: {{ B_disks_per_vm }}
        read_percent: {{ D_read_percent }}
        random_percent: {{ E_random_percent }}
        iodepth: {{ F_iodepth }}
        block_size: {{ G_block_size }}
        target_rate: {{ s_target_rate }}
{% endfor %}
{% endif %}
  - Unlimited IOPS:
      vm_group: group_0
      config_file: custom.fio
      config_variables:
        disk_size: {{ (C_vm_wss / B_disks_per_vm) | int }}
        disk_count: {{ B_disks_per_vm }}
        read_percent: {{ D_read_percent }}
        random_percent: {{ E_random_percent }}
        iodepth: {{ F_iodepth }}
        block_size: {{ G_block_size }}
        target_rate: 0

results:
  - Workload Aggregated IOPS:
      vm_group: group_0
      result_type: iops
      aggregate: sum
      report_group: performance
      report_metrics:
        - Median
      result_hint: "Higher IOPS indicates better performance."
  - Workload Aggregated Latency:
      vm_group: group_0
      result_type: latency
      result_hint: "Lower latency indicates better performance."
      report_group: performance
      aggregate: sum
      report_metrics:
        - Median
  - Cluster CPU Usage:
      metric: CpuUsage.Avg.Megahertz
      aggregate: sum
  - Cluster Network Received:
      metric: NetReceived.Avg.KilobytesPerSecond
      aggregate: sum
  - Cluster Network Transmitted:
      metric: NetTransmitted.Avg.KilobytesPerSecond
      aggregate: sum


setup:
  ## Remove known hosts file. Fixes failure of Ansible to ssh to CVMs/Hosts after foundation of a previously tested cluster.
  - experimental.Shell:
      cmd: rm -f /root/.ssh/known_hosts
  ## Clean up X-ray obects (UVMs, gold images)
  {% if cluster_cleanup == true %}
  - cluster.CleanUp: {}
  {% endif %}
  - test.Wait: {duration_secs: 10}

  {% if set_gflags == true %}

  - playbook.Run:
      filename: cvm_cluster_stop.yml
      inventory:
        - cvms
      remote_user: nutanix
      remote_pass: nutanix/4u

  - test.Wait:
      duration_secs: 30

  # Disable periodic curator scans
  - playbook.Run:
     filename: cvm_gflag_set.yml
     inventory:
       - cvms
     remote_user: nutanix
     remote_pass: nutanix/4u
     variables:
       gflags:
         /home/nutanix/config/curator.gflags:
           - name: "--curator_experimental_disable_dynamic_and_periodic_scans"
             value: "true"
         /home/nutanix/config/cerebro.gflags:
           - name: "--near_sync_test_hook_override_pre_checks_for_qa_test"
             value: "true"

  - playbook.Run:
      filename: cvm_cluster_start.yml
      inventory:
        - cvms
      remote_user: nutanix
      remote_pass: nutanix/4u

  - test.Wait:
      duration_secs: 30

  {% endif %}

{% if fresh_start == true %}

{% for curator_scan_index in range(number_of_curator_scans) %}
  ## Clean up storage with curator scan 
  - playbook.Run:
      filename: cvm_start_curator_scan_full.yml
      inventory: [cvms]
      remote_pass: nutanix/4u
      remote_user: nutanix
  - test.Wait: {duration_secs: 10}
  ## Wait for chronos jobs to be completed
  - experimental.Shell:
      cmd: rm -f /root/.ssh/known_hosts

  - playbook.Run:
      filename: cvm_wait_chronos_jobs_zero.yml
      inventory: [cvms]
      remote_pass: nutanix/4u
      remote_user: nutanix
  - test.Wait: {duration_secs: 10}
  # Run nodetool compaction to clean up metadata 
  - playbook.Run:
      filename: cvm_nodetool_compact.yml
      inventory:
        - cvms
      remote_user: nutanix
      remote_pass: nutanix/4u
{% endfor %}

# Run fstrim on all CVMs
  - playbook.Run:
      filename: fstrim.yml
      inventory:
        - cvms
      remote_user: nutanix
      remote_pass: nutanix/4u

{% endif %}

  - vm_group.CloneFromTemplate:
      vm_group_name: group_0

  - vm_group.PowerOn:
      vm_group_name: group_0

  - test.Wait:
      duration_secs: 60

  - workload.PrefillRun:
      workload_name: Custom Workload Prefill

# Wait for counters to stabilize before entering test setup phase.
  - test.Wait:
      duration_secs: 120 

run:
# Wait for Oplog/LWS to be empty.
# {% if M_wait4oplogdrain > 0 %}
  - experimental.Shell:
      cmd: rm -f /root/.ssh/known_hosts

  - playbook.Run:
      filename: wait_oplog_lws_empty.yml
      inventory:
        - cvms
      variables:
        timeout_secs: {{ M_timeout4oplogdrain }}
        wait_time: {{ M_wait4oplogdrain }}
        target_cvms: {{ tgt_cvms }}
      remote_user: nutanix
      remote_pass: nutanix/4u

  - playbook.Run:
      filename: wait_oplog_lws_empty.yml
      inventory:
        - cvms
      variables:
        timeout_secs: {{ M_timeout4oplogdrain }}
        wait_time: {{ M_wait4oplogdrain }}
        target_cvms: {{ tgt_cvms }}
      remote_user: nutanix
      remote_pass: nutanix/4u

#{% endif %}

# Protect VMs, this is supposed to start the seeding process. Need to wait.
  - playbook.Run:
      filename: protect_vms.yml
      inventory:
        - cvms
      variables:
        user: {{ pg_username }}
        passwd: {{ pg_passwd }}
        pgip: {{ Q_pgip }}
        pdname: {{ P_pd_name }}
      remote_user: nutanix
      remote_pass: nutanix/4u

# Wait for 60 secs or so for the above changes to take effect.
  - test.Wait:
      duration_secs: 60 

# Wait for replication to be completed
  - experimental.Shell:
      cmd: rm -f /root/.ssh/known_hosts

  - playbook.Run:
      filename: wait_pd_replications.yml
      inventory:
        - cvms
      variables:
        user: {{ pg_username }}
        passwd: {{ pg_passwd }}
        pgip: {{ Q_pgip }}
        pdname: {{ P_pd_name }}
        wtime: {{ O_wait4replications }}
      remote_user: nutanix
      remote_pass: nutanix/4u

# Wait for near sync active
  - experimental.Shell:
      cmd: rm -f /root/.ssh/known_hosts

  - playbook.Run:
      filename: wait_for_nearsync.yml
      inventory:
        - cvms
      variables:
        pdname: {{ P_pd_name }}
        timeout_secs: {{ O_wait4replications }}
      remote_user: nutanix
      remote_pass: nutanix/4u

{% if M_wait4oplogdrain > 0 %}
  - playbook.Run:
      filename: wait_oplog_lws_empty.yml
      inventory:
        - cvms
      variables:
        timeout_secs: {{ M_timeout4oplogdrain }}
        wait_time: {{ M_wait4oplogdrain }}
        target_cvms: {{ tgt_cvms }}
      remote_user: nutanix
      remote_pass: nutanix/4u

  - playbook.Run:
      filename: wait_oplog_lws_empty.yml
      inventory:
        - cvms
      variables:
        timeout_secs: {{ M_timeout4oplogdrain }}
        wait_time: {{ M_wait4oplogdrain }}
        target_cvms: {{ tgt_cvms }}
      remote_user: nutanix
      remote_pass: nutanix/4u
{% endif %}

  - playbook.Run:
      filename: get_cvm_cerebro_replication_duration_information.yml
      inventory:
        - cvms
      remote_user: nutanix
      remote_pass: nutanix/4u
      variables:
        log_interval: 20

  - playbook.Run:
      filename: fetch_cvm_cerebro_replication_duration_information.yml
      inventory:
        - cvms
      remote_user: nutanix
      remote_pass: nutanix/4u
      variables:
        iteration: seed

# Now seeding process is over and ready for user IO
{% if H_target_rate > 0 %}
{% for s_target_rate in range(N_start_trate, H_target_rate+K_steps, (H_target_rate/K_steps)|int) %}

  - workload.Start:
      workload_name: {{ s_target_rate }} IOPS
      runtime_secs: {{ I_runtime }}
      annotate: True

# Delete PD schedule for LWS to sync in for next clear run.
  - playbook.Run:
      filename: delete_pd_schedule.yml
      inventory:
        - cvms
      remote_user: nutanix
      remote_pass: nutanix/4u
      variables:
        user: {{ pg_username }}
        passwd: {{ pg_passwd }}
        pgip: {{ Q_pgip }}
        pdname: {{ P_pd_name }}

  - playbook.Run:
      filename: delete_pd_snapshots.yml
      inventory:
        - cvms
      remote_user: nutanix
      remote_pass: nutanix/4u
      variables:
        user: {{ pg_username }}
        passwd: {{ pg_passwd }}
        pgip: {{ Q_pgip }}
        pdname: {{ P_pd_name }}

  - playbook.Run:
      filename: delete_pd_snapshots.yml
      inventory:
        - cvms
      remote_user: nutanix
      remote_pass: nutanix/4u
      variables:
        user: {{ pg_username }}
        passwd: {{ pg_passwd }}
        pgip: {{ tgt_pgip }}
        pdname: {{ P_pd_name }}


{% if M_wait4oplogdrain > 0 %}
  - experimental.Shell:
      cmd: rm -f /root/.ssh/known_hosts

  - playbook.Run:
      filename: wait_oplog_lws_empty.yml
      inventory:
        - cvms
      variables:
        timeout_secs: {{ M_timeout4oplogdrain }}
        wait_time: {{ M_wait4oplogdrain }}
        target_cvms: {{ tgt_cvms }}
      remote_user: nutanix
      remote_pass: nutanix/4u

  - playbook.Run:
      filename: wait_oplog_lws_empty.yml
      inventory:
        - cvms
      variables:
        timeout_secs: {{ M_timeout4oplogdrain }}
        wait_time: {{ M_wait4oplogdrain }}
        target_cvms: {{ tgt_cvms }}
      remote_user: nutanix
      remote_pass: nutanix/4u
{% endif %}

  - playbook.Run:
      filename: get_cvm_cerebro_replication_duration_information.yml
      inventory:
        - cvms
      remote_user: nutanix
      remote_pass: nutanix/4u
      variables:
        log_interval: {{ I_runtime/60 + 5 }}

  - playbook.Run:
      filename: fetch_cvm_cerebro_replication_duration_information.yml
      inventory:
        - cvms
      remote_user: nutanix
      remote_pass: nutanix/4u
      variables:
        iteration: {{ s_target_rate }}

  - playbook.Run:
      filename: create_pd_schedule.yml
      inventory:
        - cvms
      remote_user: nutanix
      remote_pass: nutanix/4u
      variables:
        user: {{ pg_username }}
        passwd: {{ pg_passwd }}
        pgip: {{ Q_pgip }}
        pdname: {{ P_pd_name }}
        stype: "MINUTELY"
        nth: {{ R_nearsync_interval }}
        remote: {{ S_remote }}

# Wait for 60 secs or so for the above changes to take effect.
  - test.Wait:
      duration_secs: 60 
  - experimental.Shell:
      cmd: rm -f /root/.ssh/known_hosts

  - playbook.Run:
      filename: wait_for_nearsync.yml
      inventory:
        - cvms
      variables:
        pdname: {{ P_pd_name }}
        timeout_secs: {{ O_wait4replications }}
      remote_user: nutanix
      remote_pass: nutanix/4u

{% endfor %}
{% endif %}

  - workload.Start:
      workload_name: Unlimited IOPS
      runtime_secs: {{ I_runtime }}
      annotate: True

{% if M_wait4oplogdrain > 0 %}
  - experimental.Shell:
      cmd: rm -f /root/.ssh/known_hosts

  - playbook.Run:
      filename: wait_oplog_lws_empty.yml
      inventory:
        - cvms
      variables:
        timeout_secs: {{ M_timeout4oplogdrain }}
        wait_time: {{ M_wait4oplogdrain }}
        target_cvms: {{ tgt_cvms }}
      remote_user: nutanix
      remote_pass: nutanix/4u

  - playbook.Run:
      filename: wait_oplog_lws_empty.yml
      inventory:
        - cvms
      variables:
        timeout_secs: {{ M_timeout4oplogdrain }}
        wait_time: {{ M_wait4oplogdrain }}
        target_cvms: {{ tgt_cvms }}
      remote_user: nutanix
      remote_pass: nutanix/4u
{% endif %}

  - playbook.Run:
      filename: get_cvm_cerebro_replication_duration_information.yml
      inventory:
        - cvms
      remote_user: nutanix
      remote_pass: nutanix/4u
      variables:
        log_interval: {{ I_runtime/60 + 5 }}

  - playbook.Run:
      filename: fetch_cvm_cerebro_replication_duration_information.yml
      inventory:
        - cvms
      remote_user: nutanix
      remote_pass: nutanix/4u
      variables:
        iteration: Unlimited

teardown:

  - test.Wait:
      duration_secs: 3 
